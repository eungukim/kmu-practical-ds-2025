import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import uuid
import os
from PIL import Image
from openai.types.responses import (ResponseTextDeltaEvent, ResponseFunctionCallArgumentsDeltaEvent)
from agents import Agent, Runner
from agents import function_tool
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ìƒíƒœ ê´€ë¦¬ë¥¼ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'data_storage' not in st.session_state:
    st.session_state.data_storage = {}

if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

if 'visualization_history' not in st.session_state:
    st.session_state.visualization_history = []

if 'conversation_context' not in st.session_state:
    st.session_state.conversation_context = []

# í•¨ìˆ˜ ë„êµ¬ ì •ì˜
@function_tool
def kaggle_list_datasets(keyword: str) -> str:
    """
    ê²€ìƒ‰ ì¿¼ë¦¬ì— ë§ëŠ” Kaggle ë°ì´í„°ì…‹ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤
    """
    from kaggle.api.kaggle_api_extended import KaggleApi

    try:
        # ë°ì´í„°ì…‹ ê²€ìƒ‰
        api = KaggleApi()
        api.authenticate()
        datasets = api.dataset_list(search=keyword, sort_by="hottest")
        
        result = []
        count = 0
        for dataset in datasets:
            if count >= 10:
                break
            dataset_info = {
                "ref": dataset.ref,  # owner/dataset-name í˜•ì‹
                "title": dataset.title,
            }
            
            # ì•ˆì „í•˜ê²Œ ì†ì„± ì¶”ê°€
            if hasattr(dataset, 'size'):
                dataset_info["size"] = dataset.size
            if hasattr(dataset, 'lastUpdated'):
                dataset_info["last_updated"] = str(dataset.lastUpdated)
            if hasattr(dataset, 'downloadCount'):
                dataset_info["download_count"] = dataset.downloadCount
            if hasattr(dataset, 'voteCount'):
                dataset_info["vote_count"] = dataset.voteCount
            if hasattr(dataset, 'tags'):
                dataset_info["tags"] = dataset.tags
            if hasattr(dataset, 'usabilityRating'):
                dataset_info["usability_rating"] = dataset.usabilityRating
            
            result.append(dataset_info)
            count += 1
        
        return {
            "success": True,
            "count": len(result),
            "datasets": result
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        }

@function_tool
def kaggle_download_dataset(dataset_ref: str) -> str:
    """
    Kaggle ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤
    """
    from kaggle.api.kaggle_api_extended import KaggleApi
    api = KaggleApi()
    api.authenticate()
    try:
        # Kaggle APIë¡œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
        api.dataset_download_files(dataset_ref, path=f"datasets/{dataset_ref}/", unzip=True)
        import os
        import pandas as pd
        
        # ë‹¤ìš´ë¡œë“œ ì„±ê³µ ì‹œ ì²˜ë¦¬
        dataset_path = f"datasets/{dataset_ref}/"
        
        # ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        files = os.listdir(dataset_path)
        
        # CSV íŒŒì¼ ì°¾ê¸°
        csv_files = [f for f in files if f.endswith('.csv')]
        
        if csv_files:
            # ì²« ë²ˆì§¸ CSV íŒŒì¼ ë¡œë“œ
            first_csv = os.path.join(dataset_path, csv_files[0])
            df = pd.read_csv(first_csv)
            
            # ë°ì´í„° ì €ì¥ì†Œì— ì €ì¥
            st.session_state.data_storage[dataset_ref] = df
            
            return {
                "success": True,
                "message": f"ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ì„±ê³µ : (ì €ì¥ì†Œ key : {dataset_ref})",
                "file_loaded": csv_files[0],
                "rows": len(df),
                "columns": len(df.columns)
            }
        else:
            return {
                "success": True,
                "message": f"ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì„±ê³µí–ˆìœ¼ë‚˜ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {dataset_ref}",
                "files_available": files
            }
    except Exception as e:
        return {
            "success": False,
            "message": f"ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        }

@function_tool
def show_df_stats(key: str) -> str:
    """ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì˜ ê¸°ë³¸ í†µê³„ ì •ë³´ ì¶œë ¥
    ì €ì¥ëœ ë°ì´í„° í˜•íƒœ  (key) : Pandas DataFrame
    """
    if key not in st.session_state.data_storage:
        return "í•´ë‹¹ í‚¤ì˜ ë°ì´í„°í”„ë ˆì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
    
    df = st.session_state.data_storage[key]
    return f"""
    í–‰ ìˆ˜: {len(df)}
    ì—´ ìˆ˜: {len(df.columns)}
    ê²°ì¸¡ì¹˜ ìˆ˜: {df.isnull().sum().sum()}
    ì»¬ëŸ¼ ëª©ë¡: {df.columns.tolist()}
    ì»¬ëŸ¼ íƒ€ì…: {df.dtypes.to_dict()}
    ê¸°ì´ˆ í†µê³„: {df.describe().to_dict()}
    """

@function_tool
def random_sample(key: str, n: int) -> str:
    """ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì—ì„œ ëœë¤ ìƒ˜í”Œ ì¶”ì¶œ
    ì €ì¥ëœ ë°ì´í„° í˜•íƒœ  (key) : Pandas DataFrame
    """
    if key not in st.session_state.data_storage:
        return "í•´ë‹¹ í‚¤ì˜ ë°ì´í„°í”„ë ˆì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
    new_key = str(uuid.uuid4())
    df = st.session_state.data_storage[key]
    random_sample_df = df.sample(n)
    st.session_state.data_storage[new_key] = random_sample_df
    return f"ëœë¤ ìƒ˜í”Œ ì¶”ì¶œ ì™„ë£Œ: (ì €ì¥ì†Œ key : {new_key})"

@function_tool
def prep_df(key: str, target_column: str) -> str:
    """ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì„ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ì í•©í•œ í˜•íƒœë¡œ ì „ì²˜ë¦¬
    ì €ì¥ëœ ë°ì´í„° í˜•íƒœ  (key) : Pandas DataFrame
    ìƒˆë¡œ ì €ì¥í•  ë°ì´í„° í˜•íƒœ (new_key) : (X, y) í˜•íƒœì˜ íŠœí”Œ
    """
    new_key = str(uuid.uuid4())
    df = st.session_state.data_storage[key]
    if target_column not in df.columns:
        return f"í•´ë‹¹ ì»¬ëŸ¼ {target_column}ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    df = df[df[target_column].notna()]
    
    # ì‹¤ìˆ˜í˜• ë°ì´í„°ë§Œ ë‚¨ê¸°ê¸°
    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
    df = df[numeric_cols]
    
    if target_column not in df.columns:
        return f"íƒ€ê²Ÿ ì»¬ëŸ¼ {target_column}ì´ ì‹¤ìˆ˜í˜• ë°ì´í„°ê°€ ì•„ë‹ˆì–´ì„œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    X = df.drop(target_column, axis=1)
    y = df[target_column]
    st.session_state.data_storage[new_key] = (X, y)
    return f"ë°ì´í„°í”„ë ˆì„ ì „ì²˜ë¦¬ ì™„ë£Œ (ì‹¤ìˆ˜í˜• ë°ì´í„°ë§Œ í¬í•¨): (ì €ì¥ì†Œ key : {new_key})"

@function_tool
def visualize_tsne(key: str) -> str:
    """
    ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì„ t-SNEë¡œ ì‹œê°í™”
    ì €ì¥ëœ ë°ì´í„° í˜•íƒœ (key) : (X, y) í˜•íƒœì˜ íŠœí”Œ
    ìƒˆë¡œ ì €ì¥í•  ë°ì´í„° í˜•íƒœ (new_key) : Plotly ê°ì²´
    ì‹œê°í™” ê²°ê³¼ëŠ” Streamlitì— í‘œì‹œë©ë‹ˆë‹¤.
    """
    from utils import TSNEVisualizer
    import plotly.io as pio
    import os
    
    new_key = str(uuid.uuid4())
    
    # íŒŒì¼ ì €ì¥ì„ ìœ„í•œ ê²½ë¡œ ì„¤ì •
    output_dir = "visualization_output"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"tsne_{new_key}.png")
    
    (X, y) = st.session_state.data_storage[key]
    
    try:
        fig = TSNEVisualizer.visualize(X, y, perplexity=5)
        
        # PNG íŒŒì¼ë¡œ ì €ì¥
        fig.write_image(output_path)
        
        # ì„¸ì…˜ ìƒíƒœì— ì‹œê°í™” ê°ì²´ì™€ ê²½ë¡œ ì €ì¥
        st.session_state.data_storage[new_key] = fig
        st.session_state.visualization_history.append({
            "type": "tsne",
            "key": new_key,
            "path": output_path,
            "title": "t-SNE ì‹œê°í™”"
        })
        
        return f"t-SNE ì‹œê°í™” ì™„ë£Œ: (ì €ì¥ì†Œ key : {new_key}, íŒŒì¼ ê²½ë¡œ: {output_path})"
    except Exception as e:
        return f"t-SNE ì‹œê°í™” ì‹¤íŒ¨: {str(e)}"

@function_tool
def under_resample(key: str) -> str:
    """
    ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì„ ì–¸ë”ë¦¬ìƒ˜í”Œë§
    ì €ì¥ëœ ë°ì´í„° í˜•íƒœ  (key) : (X, y) í˜•íƒœì˜ íŠœí”Œ
    ìƒˆë¡œ ì €ì¥í•  ë°ì´í„° í˜•íƒœ (new_key) : (X, y) í˜•íƒœì˜ íŠœí”Œ
    """
    from utils import ImbalancedDataAnalyzer
    new_key = str(uuid.uuid4())
    (X, y) = st.session_state.data_storage[key]
    analyzer = ImbalancedDataAnalyzer(X, y)
    X_resampled, y_resampled = analyzer.random_undersample()
    st.session_state.data_storage[new_key] = (X_resampled, y_resampled)
    return f"ì–¸ë”ë¦¬ìƒ˜í”Œë§ ì™„ë£Œ: (ì €ì¥ì†Œ key : {new_key})"

@function_tool
def smote_resample(key: str) -> str:
    """
    ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì„ SMOTEë¡œ ì˜¤ë²„ìƒ˜í”Œë§
    ì €ì¥ëœ ë°ì´í„° í˜•íƒœ  (key) : (X, y) í˜•íƒœì˜ íŠœí”Œ
    ìƒˆë¡œ ì €ì¥í•  ë°ì´í„° í˜•íƒœ (new_key) : (X, y) í˜•íƒœì˜ íŠœí”Œ
    """
    from utils import ImbalancedDataAnalyzer
    new_key = str(uuid.uuid4())
    (X, y) = st.session_state.data_storage[key]
    analyzer = ImbalancedDataAnalyzer(X, y)
    try:
        X_resampled, y_resampled = analyzer.smote_oversample()
        st.session_state.data_storage[new_key] = (X_resampled, y_resampled)
        return f"SMOTE ì˜¤ë²„ìƒ˜í”Œë§ ì™„ë£Œ: (ì €ì¥ì†Œ key : {new_key})"
    except Exception as e:
        return f"SMOTE ì˜¤ë²„ìƒ˜í”Œë§ ì‹¤íŒ¨: {str(e)}"

@function_tool
def isolation_forest(key: str, contamination: float) -> str:
    """
    ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì„ Isolation Forestë¡œ ë¶„ì„
    ì €ì¥ëœ ë°ì´í„° í˜•íƒœ  (key) : (X, y) í˜•íƒœì˜ íŠœí”Œ
    ìƒˆë¡œ ì €ì¥í•  ë°ì´í„° í˜•íƒœ (new_key) : (Plotly ê°ì²´, y, score) í˜•íƒœì˜ íŠœí”Œ
    ì‹œê°í™” ê²°ê³¼ëŠ” Streamlitì— í‘œì‹œë©ë‹ˆë‹¤.
    """
    from sklearn.ensemble import IsolationForest
    import plotly.express as px
    import os
    
    new_key = str(uuid.uuid4())
    
    # íŒŒì¼ ì €ì¥ì„ ìœ„í•œ ê²½ë¡œ ì„¤ì •
    output_dir = "visualization_output"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"isolation_forest_{new_key}.png")
    
    (X, y) = st.session_state.data_storage[key]
    model = IsolationForest(contamination=contamination, max_samples='auto')
    model.fit(X)
    score = model.score_samples(X)
    score = -1 * score
    fig = px.histogram(x=score, nbins=100, labels={'x':'Score'}, title="ì´ìƒì¹˜ ì ìˆ˜ ë¶„í¬")
    fig.update_layout(width=600, height=400)
    
    # PNG íŒŒì¼ë¡œ ì €ì¥
    fig.write_image(output_path)
    
    # ê·¸ë˜í”„ì™€ í•¨ê»˜ yì™€ scoreë„ ì €ì¥
    st.session_state.data_storage[new_key] = (fig, y, score)
    
    # ì‹œê°í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    st.session_state.visualization_history.append({
        "type": "isolation_forest",
        "key": new_key,
        "path": output_path,
        "title": "Isolation Forest ë¶„ì„"
    })
    
    # ì¶”ê°€ë¡œ precision-recall ê³¡ì„ ì„ ìœ„í•œ ë³„ë„ í‚¤ ì €ì¥
    pr_key = str(uuid.uuid4())
    st.session_state.data_storage[pr_key] = (y, score)
    
    return f"Isolation Forest ë¶„ì„ ì™„ë£Œ: (ì €ì¥ì†Œ key : {new_key}, PR ê³¡ì„ ìš© key: {pr_key}, íŒŒì¼ ê²½ë¡œ: {output_path})"

@function_tool
def prec_rec_f1_curve(key: str):
    """
    ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì„ Precision, Recall, F1 Score ê³¡ì„ ìœ¼ë¡œ ì‹œê°í™”
    ì €ì¥ëœ ë°ì´í„° í˜•íƒœ  (key) : (y, score) í˜•íƒœì˜ íŠœí”Œ
    ìƒˆë¡œ ì €ì¥í•  ë°ì´í„° í˜•íƒœ (new_key) : (precision, recall, f1, thresholds) í˜•íƒœì˜ íŠœí”Œ
    ì‹œê°í™” ê²°ê³¼ëŠ” Streamlitì— í‘œì‹œë©ë‹ˆë‹¤.
    """
    import numpy as np
    import plotly.graph_objects as go
    from sklearn.metrics import precision_recall_curve
    import os
    
    new_key = str(uuid.uuid4())
    y, score = st.session_state.data_storage[key]
    
    precision, recall, thresholds = precision_recall_curve(y, score, pos_label=1)
    f1 = 2 * (precision * recall) / (precision + recall + 1e-10)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€

    fig = go.Figure()
    fig.add_trace(go.Scatter(x=thresholds, y=np.delete(precision, -1), mode='lines', name='precision'))
    fig.add_trace(go.Scatter(x=thresholds, y=np.delete(recall, -1), mode='lines', name='recall'))
    fig.add_trace(go.Scatter(x=thresholds, y=np.delete(f1, -1), mode='lines', name='f1'))
    fig.update_layout(
        title='Anomaly Scoreì— ë”°ë¥¸ Precision, Recall, F1 Score',
        xaxis_title='Anomaly Score',
        yaxis_title='Score',
        legend_title='Score Type'
    )
    
    # íŒŒì¼ ì €ì¥ì„ ìœ„í•œ ê²½ë¡œ ì„¤ì •
    output_dir = "visualization_output"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"prf1_{new_key}.png")
    
    # PNG íŒŒì¼ë¡œ ì €ì¥
    fig.write_image(output_path)
    
    # ì„¸ì…˜ ìƒíƒœì— ê²°ê³¼ ì €ì¥
    st.session_state.data_storage[new_key] = (precision, recall, f1, thresholds)
    
    # ì‹œê°í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    st.session_state.visualization_history.append({
        "type": "prf1",
        "key": new_key,
        "path": output_path,
        "title": "Precision-Recall-F1 ê³¡ì„ "
    })
    
    return f"Precision-Recall-F1 ê³¡ì„  ì‹œê°í™” ì™„ë£Œ: (ì €ì¥ì†Œ key : {new_key}, íŒŒì¼ ê²½ë¡œ: {output_path})"

@function_tool
def show_tool_list() -> str:
    """
    í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì„ ì¶œë ¥
    """
    tool_names = [
        "show_tool_list",
        "kaggle_list_datasets",
        "kaggle_download_dataset",
        "show_df_stats",
        "random_sample",
        "prep_df",
        "visualize_tsne",
        "under_resample",
        "smote_resample",
        "isolation_forest",
        "prec_rec_f1_curve",
    ]
    return f"í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡: {', '.join(tool_names)}"

# OpenAI Agent ì„¤ì •
def create_agent():
    return Agent(
        name="ë°ì´í„°ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸",
        instructions=(
            "ë„ˆëŠ” ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì•¼. "
            "ê°€ëŠ¥í•œ í•œ í•­ìƒ ì œê³µëœ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¸°ì–µí•´ì•¼ í•´. "
            "ë„ˆê°€ ì‚¬ìš©ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì„ ë¨¼ì € ì¶œë ¥í•´ë†“ê³  ì‹œì‘í•´. show_tool_list ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë©´ ë¨. "
            "ìì‹ ì˜ ì§€ì‹ì— ë„ˆë¬´ ì˜ì¡´í•˜ì§€ ë§ê³  ëŒ€ì‹  ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. "
            "python ì½”ë“œë¥¼ ì§ì ‘êµ¬í˜„í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ë„êµ¬ì— ì˜ì¡´í•˜ì„¸ìš”. "
            "t-SNE ì‹œê°í™”ë‚˜ ë‹¤ë¥¸ ì‹œê°í™”ë¥¼ ì‚¬ìš©í•  ë•Œ ìƒ˜í”Œ ìˆ˜ê°€ ì ì–´ 'perplexity must be less than n_samples' ë“±ì˜ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´, "
            "í•´ë‹¹ ì‹œê°í™” ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³  ì‚¬ìš©ìì—ê²Œ ìƒ˜í”Œ ìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬ ì‹œê°í™”ê°€ ë¶ˆê°€ëŠ¥í•˜ë‹¤ê³  ì•Œë ¤ì£¼ì„¸ìš”."
            "í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."
        ),
        model="gpt-4o",
        tools=[
            show_tool_list,
            kaggle_list_datasets,
            kaggle_download_dataset,
            show_df_stats,
            random_sample,
            prep_df,
            visualize_tsne,
            under_resample,
            smote_resample,
            isolation_forest,
            prec_rec_f1_curve,
        ]
    )

# ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ê´€ë¦¬í•˜ëŠ” í•¨ìˆ˜
def update_conversation_context(message, is_user=False):
    # ëŒ€í™” ë‚´ìš© ì¶”ê°€
    st.session_state.chat_history.append({"message": message, "is_user": is_user})
    
    # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
    role = "User" if is_user else "Assistant"
    st.session_state.conversation_context.append({"role": role, "content": message})

# ì—ì´ì „íŠ¸ì™€ ì±„íŒ… ì²˜ë¦¬ í•¨ìˆ˜
async def process_message(user_input):
    agent = create_agent()
    
    # ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
    update_conversation_context(user_input, is_user=True)
    
    # ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—ì´ì „íŠ¸ ì§€ì‹œì‚¬í•­ ì¤€ë¹„
    context = ""
    for msg in st.session_state.conversation_context:
        context += f"{msg['role']}: {msg['content']}\n"
    
    response = Runner.run_streamed(
        starting_agent=agent,
        max_turns=20,
        input=user_input,
        context=context  # ì „ì²´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬
    )
    
    answer = ""
    async for event in response.stream_events():
        if event.type == "raw_response_event":
            if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):
                # ë„êµ¬ í˜¸ì¶œì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ë§¤ê°œë³€ìˆ˜
                pass
            elif isinstance(event.data, ResponseTextDeltaEvent):
                # ìŠ¤íŠ¸ë¦¬ë°ëœ ìµœì¢… ì‘ë‹µ í† í°
                answer += event.data.delta
                yield event.data.delta
        elif event.type == "run_item_stream_event":
            if event.name == "tool_called":
                # ë„êµ¬ í˜¸ì¶œ ë°œìƒ ì‹œ UIì— í‘œì‹œ
                tool_name = event.item.raw_item.name
                tool_args = event.item.raw_item.arguments
                tool_message = f"ë„êµ¬ í˜¸ì¶œ: {tool_name}({tool_args})"
                update_conversation_context(tool_message, is_user=False)
                yield f"[ë„êµ¬ í˜¸ì¶œì¤‘: {tool_name}...] "
            elif event.name == "tool_output":
                # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼
                tool_output = event.item.raw_item['output']
                if isinstance(tool_output, dict) and 'success' in tool_output:
                    output_msg = tool_output.get('message', str(tool_output))
                else:
                    output_msg = str(tool_output)
                update_conversation_context(f"ë„êµ¬ ì¶œë ¥: {output_msg}", is_user=False)
                yield f"[ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ] "
    
    # ìµœì¢… ì‘ë‹µì„ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
    update_conversation_context(answer, is_user=False)

# Streamlit UI
st.title("ML ì´ìƒíƒì§€ ì–´ì‹œìŠ¤í„´íŠ¸")

# ì‚¬ì´ë“œë°”ì— ë„êµ¬ ëª©ë¡ í‘œì‹œ
with st.sidebar:
    st.header("ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡")
    st.markdown("""
    1. **show_tool_list**: ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ì¶œë ¥
    2. **kaggle_list_datasets**: ê²€ìƒ‰ ì¿¼ë¦¬ì— ë§ëŠ” Kaggle ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ
    3. **kaggle_download_dataset**: Kaggle ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
    4. **show_df_stats**: ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì˜ ê¸°ë³¸ í†µê³„ ì •ë³´ ì¶œë ¥
    5. **random_sample**: ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì—ì„œ ëœë¤ ìƒ˜í”Œ ì¶”ì¶œ
    6. **prep_df**: ë°ì´í„°í”„ë ˆì„ì„ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ì í•©í•œ í˜•íƒœë¡œ ì „ì²˜ë¦¬
    7. **visualize_tsne**: ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì„ t-SNEë¡œ ì‹œê°í™”
    8. **under_resample**: ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì„ ì–¸ë”ë¦¬ìƒ˜í”Œë§
    9. **smote_resample**: ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì„ SMOTEë¡œ ì˜¤ë²„ìƒ˜í”Œë§
    10. **isolation_forest**: ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì„ Isolation Forestë¡œ ë¶„ì„
    11. **prec_rec_f1_curve**: ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì„ Precision, Recall, F1 Score ê³¡ì„ ìœ¼ë¡œ ì‹œê°í™”
    """)
    
    st.header("ì‹œê°í™” íˆìŠ¤í† ë¦¬")
    if st.session_state.visualization_history:
        for idx, viz in enumerate(st.session_state.visualization_history):
            if st.button(f"{viz['title']} #{idx+1}", key=f"viz_btn_{idx}"):
                st.session_state.selected_viz = viz

# ë©”ì¸ ì˜ì—­ì— ì±„íŒ… UI í‘œì‹œ
for message in st.session_state.chat_history:
    with st.chat_message("user" if message["is_user"] else "assistant"):
        st.write(message["message"])

# ì‹œê°í™” ê²°ê³¼ í‘œì‹œ
if 'selected_viz' in st.session_state:
    viz = st.session_state.selected_viz
    st.subheader(viz['title'])
    try:
        image = Image.open(viz['path'])
        st.image(image, caption=viz['title'])
    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_input = st.chat_input("ë°ì´í„° ë¶„ì„ ìš”ì²­ì„ ì…ë ¥í•˜ì„¸ìš”...")

if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ UIì— í‘œì‹œ
    with st.chat_message("user"):
        st.write(user_input)
    
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰ì„ ìœ„í•œ ì½”ë£¨í‹´ ë˜í¼
        import asyncio
        
        async def run_chat():
            full_response = ""  # ì—¬ê¸°ì„œ ì§€ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸
            async for chunk in process_message(user_input):
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
        
        asyncio.run(run_chat())

# ì‹œì‘ ë©”ì‹œì§€ í‘œì‹œ
if not st.session_state.chat_history:
    st.info("ğŸ‘‹ ì•ˆë…•í•˜ì„¸ìš”! ML ì´ìƒíƒì§€ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì–´ë–¤ ë¶„ì„ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    st.markdown("""
    **ì˜ˆì‹œ ì§ˆë¬¸:**
    - ì‚¬ê¸° ì´ìƒíƒì§€ ê´€ë ¨ ë°ì´í„°ì…‹ì„ ìºê¸€ì—ì„œ ì¡°íšŒí•´ì¤˜
    - ì²«ë²ˆì§¸ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  3000ê°œ ë ˆì½”ë“œë§Œ ëœë¤ ìƒ˜í”Œë§í•´ì¤˜
    - ìƒ˜í”Œë§í•œ ë°ì´í„°ì˜ ê¸°ì´ˆ í†µê³„ëŸ‰ì„ ë³´ì—¬ì¤˜
    - ë°ì´í„°ë¥¼ TSNEë¡œ ì‹œê°í™”í•´ì¤˜
    """) 